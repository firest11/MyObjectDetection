{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying DETR Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports here\n",
    "import argparse\n",
    "import datetime\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from DeformableDETR import datasets\n",
    "import DeformableDETR.util.misc as utils\n",
    "import DeformableDETR.datasets.samplers as samplers\n",
    "from DeformableDETR.datasets import build_dataset, get_coco_api_from_dataset\n",
    "from DeformableDETR.engine import evaluate, train_one_epoch\n",
    "from DeformableDETR.models import build_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Args-Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args_parser():\n",
    "    parser = argparse.ArgumentParser('Deformable DETR Detector', add_help=False)\n",
    "    # Optimization Arguments\n",
    "    parser.add_argument('--lr', default=2e-4, type=float)\n",
    "    parser.add_argument('--lr_backbone_names', default=[\"backbone.0\"], type=str, nargs='+')\n",
    "    parser.add_argument('--lr_backbone', default=2e-5, type=float)\n",
    "    parser.add_argument('--lr_linear_proj_names', default=['reference_points', 'sampling_offsets'], type=str, nargs='+')\n",
    "    parser.add_argument('--lr_linear_proj_mult', default=0.1, type=float)\n",
    "    parser.add_argument('--batch_size', default=2, type=int)\n",
    "    parser.add_argument('--weight_decay', default=1e-4, type=float)\n",
    "    parser.add_argument('--epochs', default=50, type=int)\n",
    "    parser.add_argument('--lr_drop', default=40, type=int)\n",
    "    parser.add_argument('--lr_drop_epochs', default=None, type=int, nargs='+')\n",
    "    parser.add_argument('--clip_max_norm', default=0.1, type=float,\n",
    "                        help='gradient clipping max norm')\n",
    "\n",
    "\n",
    "    parser.add_argument('--sgd', action='store_true')\n",
    "\n",
    "    # Variants of Deformable DETR\n",
    "    parser.add_argument('--with_box_refine', default=False, action='store_true')\n",
    "    parser.add_argument('--two_stage', default=False, action='store_true')\n",
    "\n",
    "    # Model parameters\n",
    "    parser.add_argument('--frozen_weights', type=str, default=None,\n",
    "                        help=\"Path to the pretrained model. If set, only the mask head will be trained\")\n",
    "\n",
    "    # * Backbone\n",
    "    parser.add_argument('--backbone', default='resnet50', type=str,\n",
    "                        help=\"Name of the convolutional backbone to use\")\n",
    "    parser.add_argument('--dilation', action='store_true',\n",
    "                        help=\"If true, we replace stride with dilation in the last convolutional block (DC5)\")\n",
    "    parser.add_argument('--position_embedding', default='sine', type=str, choices=('sine', 'learned'),\n",
    "                        help=\"Type of positional embedding to use on top of the image features\")\n",
    "    parser.add_argument('--position_embedding_scale', default=2 * np.pi, type=float,\n",
    "                        help=\"position / size * scale\")\n",
    "    parser.add_argument('--num_feature_levels', default=4, type=int, help='number of feature levels')\n",
    "\n",
    "    # * Transformer\n",
    "    parser.add_argument('--enc_layers', default=6, type=int,\n",
    "                        help=\"Number of encoding layers in the transformer\")\n",
    "    parser.add_argument('--dec_layers', default=6, type=int,\n",
    "                        help=\"Number of decoding layers in the transformer\")\n",
    "    parser.add_argument('--dim_feedforward', default=1024, type=int,\n",
    "                        help=\"Intermediate size of the feedforward layers in the transformer blocks\")\n",
    "    parser.add_argument('--hidden_dim', default=256, type=int,\n",
    "                        help=\"Size of the embeddings (dimension of the transformer)\")\n",
    "    parser.add_argument('--dropout', default=0.1, type=float,\n",
    "                        help=\"Dropout applied in the transformer\")\n",
    "    parser.add_argument('--nheads', default=8, type=int,\n",
    "                        help=\"Number of attention heads inside the transformer's attentions\")\n",
    "    parser.add_argument('--num_queries', default=300, type=int,\n",
    "                        help=\"Number of query slots\")\n",
    "    parser.add_argument('--dec_n_points', default=4, type=int)\n",
    "    parser.add_argument('--enc_n_points', default=4, type=int)\n",
    "\n",
    "    # * Segmentation\n",
    "    parser.add_argument('--masks', action='store_true',\n",
    "                        help=\"Train segmentation head if the flag is provided\")\n",
    "\n",
    "    # Loss\n",
    "    parser.add_argument('--no_aux_loss', dest='aux_loss', action='store_false',\n",
    "                        help=\"Disables auxiliary decoding losses (loss at each layer)\")\n",
    "\n",
    "    # * Matcher\n",
    "    parser.add_argument('--set_cost_class', default=2, type=float,\n",
    "                        help=\"Class coefficient in the matching cost\")\n",
    "    parser.add_argument('--set_cost_bbox', default=5, type=float,\n",
    "                        help=\"L1 box coefficient in the matching cost\")\n",
    "    parser.add_argument('--set_cost_giou', default=2, type=float,\n",
    "                        help=\"giou box coefficient in the matching cost\")\n",
    "\n",
    "    # * Loss coefficients\n",
    "    parser.add_argument('--mask_loss_coef', default=1, type=float)\n",
    "    parser.add_argument('--dice_loss_coef', default=1, type=float)\n",
    "    parser.add_argument('--cls_loss_coef', default=2, type=float)\n",
    "    parser.add_argument('--bbox_loss_coef', default=5, type=float)\n",
    "    parser.add_argument('--giou_loss_coef', default=2, type=float)\n",
    "    parser.add_argument('--focal_alpha', default=0.25, type=float)\n",
    "\n",
    "    # dataset parameters\n",
    "    parser.add_argument('--dataset_file', default='coco')\n",
    "    parser.add_argument('--coco_path', default='./data/coco', type=str)\n",
    "    parser.add_argument('--coco_panoptic_path', type=str)\n",
    "    parser.add_argument('--remove_difficult', action='store_true')\n",
    "\n",
    "    parser.add_argument('--output_dir', default='',\n",
    "                        help='path where to save, empty for no saving')\n",
    "    parser.add_argument('--device', default='cuda',\n",
    "                        help='device to use for training / testing')\n",
    "    parser.add_argument('--seed', default=42, type=int)\n",
    "    parser.add_argument('--resume', default='', help='resume from checkpoint')\n",
    "    parser.add_argument('--start_epoch', default=0, type=int, metavar='N',\n",
    "                        help='start epoch')\n",
    "    parser.add_argument('--eval', action='store_true')\n",
    "    parser.add_argument('--num_workers', default=2, type=int)\n",
    "    parser.add_argument('--cache_mode', default=False, action='store_true', help='whether to cache images on memory')\n",
    "\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    utils.init_distributed_mode(args)\n",
    "    print(\"git:\\n  {}\\n\".format(utils.get_sha()))\n",
    "\n",
    "    if args.frozen_weights is not None:\n",
    "        assert args.masks, \"Frozen training is meant for segmentation only\"\n",
    "    print(args)\n",
    "\n",
    "    device = torch.device(args.device)\n",
    "\n",
    "    # fix the seed for reproducibility\n",
    "    seed = args.seed + utils.get_rank()\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "    model, criterion, postprocessors = build_model(args)\n",
    "    model.to(device)\n",
    "\n",
    "    model_without_ddp = model\n",
    "    n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print('number of params:', n_parameters)\n",
    "\n",
    "    dataset_train = build_dataset(image_set='train', args=args)\n",
    "    dataset_val = build_dataset(image_set='val', args=args)\n",
    "\n",
    "    if args.distributed:\n",
    "        if args.cache_mode:\n",
    "            sampler_train = samplers.NodeDistributedSampler(dataset_train)\n",
    "            sampler_val = samplers.NodeDistributedSampler(dataset_val, shuffle=False)\n",
    "        else:\n",
    "            sampler_train = samplers.DistributedSampler(dataset_train)\n",
    "            sampler_val = samplers.DistributedSampler(dataset_val, shuffle=False)\n",
    "    else:\n",
    "        sampler_train = torch.utils.data.RandomSampler(dataset_train)\n",
    "        sampler_val = torch.utils.data.SequentialSampler(dataset_val)\n",
    "\n",
    "    batch_sampler_train = torch.utils.data.BatchSampler(\n",
    "        sampler_train, args.batch_size, drop_last=True)\n",
    "\n",
    "    data_loader_train = DataLoader(dataset_train, batch_sampler=batch_sampler_train,\n",
    "                                   collate_fn=utils.collate_fn, num_workers=args.num_workers,\n",
    "                                   pin_memory=True)\n",
    "    data_loader_val = DataLoader(dataset_val, args.batch_size, sampler=sampler_val,\n",
    "                                 drop_last=False, collate_fn=utils.collate_fn, num_workers=args.num_workers,\n",
    "                                 pin_memory=True)\n",
    "\n",
    "    # lr_backbone_names = [\"backbone.0\", \"backbone.neck\", \"input_proj\", \"transformer.encoder\"]\n",
    "    def match_name_keywords(n, name_keywords):\n",
    "        out = False\n",
    "        for b in name_keywords:\n",
    "            if b in n:\n",
    "                out = True\n",
    "                break\n",
    "        return out\n",
    "\n",
    "    for n, p in model_without_ddp.named_parameters():\n",
    "        print(n)\n",
    "\n",
    "    param_dicts = [\n",
    "        {\n",
    "            \"params\":\n",
    "                [p for n, p in model_without_ddp.named_parameters()\n",
    "                 if not match_name_keywords(n, args.lr_backbone_names) and not match_name_keywords(n, args.lr_linear_proj_names) and p.requires_grad],\n",
    "            \"lr\": args.lr,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [p for n, p in model_without_ddp.named_parameters() if match_name_keywords(n, args.lr_backbone_names) and p.requires_grad],\n",
    "            \"lr\": args.lr_backbone,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [p for n, p in model_without_ddp.named_parameters() if match_name_keywords(n, args.lr_linear_proj_names) and p.requires_grad],\n",
    "            \"lr\": args.lr * args.lr_linear_proj_mult,\n",
    "        }\n",
    "    ]\n",
    "    if args.sgd:\n",
    "        optimizer = torch.optim.SGD(param_dicts, lr=args.lr, momentum=0.9,\n",
    "                                    weight_decay=args.weight_decay)\n",
    "    else:\n",
    "        optimizer = torch.optim.AdamW(param_dicts, lr=args.lr,\n",
    "                                      weight_decay=args.weight_decay)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, args.lr_drop)\n",
    "\n",
    "    if args.distributed:\n",
    "        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu])\n",
    "        model_without_ddp = model.module\n",
    "\n",
    "    if args.dataset_file == \"coco_panoptic\":\n",
    "        # We also evaluate AP during panoptic training, on original coco DS\n",
    "        coco_val = datasets.coco.build(\"val\", args)\n",
    "        base_ds = get_coco_api_from_dataset(coco_val)\n",
    "    else:\n",
    "        base_ds = get_coco_api_from_dataset(dataset_val)\n",
    "\n",
    "    if args.frozen_weights is not None:\n",
    "        checkpoint = torch.load(args.frozen_weights, map_location='cpu')\n",
    "        model_without_ddp.detr.load_state_dict(checkpoint['model'])\n",
    "\n",
    "    output_dir = Path(args.output_dir)\n",
    "    if args.resume:\n",
    "        if args.resume.startswith('https'):\n",
    "            checkpoint = torch.hub.load_state_dict_from_url(\n",
    "                args.resume, map_location='cpu', check_hash=True)\n",
    "        else:\n",
    "            checkpoint = torch.load(args.resume, map_location='cpu')\n",
    "        missing_keys, unexpected_keys = model_without_ddp.load_state_dict(checkpoint['model'], strict=False)\n",
    "        unexpected_keys = [k for k in unexpected_keys if not (k.endswith('total_params') or k.endswith('total_ops'))]\n",
    "        if len(missing_keys) > 0:\n",
    "            print('Missing Keys: {}'.format(missing_keys))\n",
    "        if len(unexpected_keys) > 0:\n",
    "            print('Unexpected Keys: {}'.format(unexpected_keys))\n",
    "        if not args.eval and 'optimizer' in checkpoint and 'lr_scheduler' in checkpoint and 'epoch' in checkpoint:\n",
    "            import copy\n",
    "            p_groups = copy.deepcopy(optimizer.param_groups)\n",
    "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "            for pg, pg_old in zip(optimizer.param_groups, p_groups):\n",
    "                pg['lr'] = pg_old['lr']\n",
    "                pg['initial_lr'] = pg_old['initial_lr']\n",
    "            print(optimizer.param_groups)\n",
    "            lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
    "            # todo: this is a hack for doing experiment that resume from checkpoint and also modify lr scheduler (e.g., decrease lr in advance).\n",
    "            args.override_resumed_lr_drop = True\n",
    "            if args.override_resumed_lr_drop:\n",
    "                print('Warning: (hack) args.override_resumed_lr_drop is set to True, so args.lr_drop would override lr_drop in resumed lr_scheduler.')\n",
    "                lr_scheduler.step_size = args.lr_drop\n",
    "                lr_scheduler.base_lrs = list(map(lambda group: group['initial_lr'], optimizer.param_groups))\n",
    "            lr_scheduler.step(lr_scheduler.last_epoch)\n",
    "            args.start_epoch = checkpoint['epoch'] + 1\n",
    "        # check the resumed model\n",
    "        if not args.eval:\n",
    "            test_stats, coco_evaluator = evaluate(\n",
    "                model, criterion, postprocessors, data_loader_val, base_ds, device, args.output_dir\n",
    "            )\n",
    "    \n",
    "    if args.eval:\n",
    "        test_stats, coco_evaluator = evaluate(model, criterion, postprocessors,\n",
    "                                              data_loader_val, base_ds, device, args.output_dir)\n",
    "        if args.output_dir:\n",
    "            utils.save_on_master(coco_evaluator.coco_eval[\"bbox\"].eval, output_dir / \"eval.pth\")\n",
    "        return\n",
    "\n",
    "    print(\"Start training\")\n",
    "    start_time = time.time()\n",
    "    for epoch in range(args.start_epoch, args.epochs):\n",
    "        if args.distributed:\n",
    "            sampler_train.set_epoch(epoch)\n",
    "        train_stats = train_one_epoch(\n",
    "            model, criterion, data_loader_train, optimizer, device, epoch, args.clip_max_norm)\n",
    "        lr_scheduler.step()\n",
    "        if args.output_dir:\n",
    "            checkpoint_paths = [output_dir / 'checkpoint.pth']\n",
    "            # extra checkpoint before LR drop and every 5 epochs\n",
    "            if (epoch + 1) % args.lr_drop == 0 or (epoch + 1) % 5 == 0:\n",
    "                checkpoint_paths.append(output_dir / f'checkpoint{epoch:04}.pth')\n",
    "            for checkpoint_path in checkpoint_paths:\n",
    "                utils.save_on_master({\n",
    "                    'model': model_without_ddp.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'lr_scheduler': lr_scheduler.state_dict(),\n",
    "                    'epoch': epoch,\n",
    "                    'args': args,\n",
    "                }, checkpoint_path)\n",
    "\n",
    "        test_stats, coco_evaluator = evaluate(\n",
    "            model, criterion, postprocessors, data_loader_val, base_ds, device, args.output_dir\n",
    "        )\n",
    "\n",
    "        log_stats = {**{f'train_{k}': v for k, v in train_stats.items()},\n",
    "                     **{f'test_{k}': v for k, v in test_stats.items()},\n",
    "                     'epoch': epoch,\n",
    "                     'n_parameters': n_parameters}\n",
    "\n",
    "        if args.output_dir and utils.is_main_process():\n",
    "            with (output_dir / \"log.txt\").open(\"a\") as f:\n",
    "                f.write(json.dumps(log_stats) + \"\\n\")\n",
    "\n",
    "            # for evaluation logs\n",
    "            if coco_evaluator is not None:\n",
    "                (output_dir / 'eval').mkdir(exist_ok=True)\n",
    "                if \"bbox\" in coco_evaluator.coco_eval:\n",
    "                    filenames = ['latest.pth']\n",
    "                    if epoch % 50 == 0:\n",
    "                        filenames.append(f'{epoch:03}.pth')\n",
    "                    for name in filenames:\n",
    "                        torch.save(coco_evaluator.coco_eval[\"bbox\"].eval,\n",
    "                                   output_dir / \"eval\" / name)\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "    print('Training time {}'.format(total_time_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
